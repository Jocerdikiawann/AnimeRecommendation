# -*- coding: utf-8 -*-
"""Proyek_Akhir_Recommender_System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QtU1_W6egRWNSdHYfAN0nHYWJZFFcbny

# Data loading

Mengimport data menggunakan kaggle.
"""

from google.colab import files
files.upload()

!rm -r ~/.kaggle
!mkdir ~/.kaggle
!mv ./kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets list

!kaggle datasets download "CooperUnion/anime-recommendations-database" -p /content/sample_data --unzip

"""# Data understanding

Terdapat 2 file yaitu :   
1. anime.csv
2. rating.csv
"""

import pandas as pd

DB_PATH="/content/sample_data"

anime = pd.read_csv(f"{DB_PATH}/anime.csv")
rating = pd.read_csv(f"{DB_PATH}/rating.csv")

print(f"Jumlah anime pada dataset : {len(anime['anime_id'].unique())}")
print(f"Jumlah users pada dataset : {len(rating['user_id'].unique())}")
print(f"Jumlah rating yang diberikan oleh user : {len(rating['user_id'].unique())}")

"""# Univariate Exploratory Data Analysis

Variabel pada dataset anime recommendation dataset:
* anime : Merupakan informasi tentang anime
* ratings : rating yang diberikan oleh pengguna ke anime yang ditontonnya.

## Anime
"""

anime.info()

"""Terdapat 8 variabel pada books, 
* anime_Id : id dari sebuah anime.
* name: judul dari anime.
* genre: genre dari sebuah anime
* type: type dari anime contoh : movie,tv,dsb.
* episodes: jumlah episode.
* members: jumlah member

Melihat berapa banyak entri unik berdasarkan anime, dan seberapa banyak type dan genre pada anime.
"""

print(f"Banyak data : {len(anime['anime_id'].unique())}")
print(f"Jumlah type : {len(anime['type'].unique())}")
print(f"Jumlah genre : {len(anime['genre'].unique())}")

"""## Rating dan user

Melihat jumlah user
"""

print(f"Jumlah user penonton anime : {len(rating['user_id'].unique())}")

"""Melihat fitur pada variabel rating."""

rating.info()

"""Terdapat user_id, anime_id, dan rating sebagai fitur di vairabel rating.

Mengecek fitur null pada variabel rating
"""

rating.isnull().sum()

"""Melihat berapa pengguna yang memberikan rating, jumlah anime, dan jumlah rating."""

print('Jumlah User-ID: ', len(rating['user_id'].unique()))
print('Jumlah anime yang diberikan rating: ', len(rating['anime_id'].unique()))
print('Jumlah data rating: ', len(rating['rating']))

"""#Data Preprocessing

## Menggabungkan seluruh anime

Mengidentifikasi berapa jumlah seluruh anime pada dataset.
"""

import numpy as np

anime_all = np.concatenate((
    anime['anime_id'].unique(),
    rating['anime_id'].unique(),
))

anime_all = np.sort(np.unique(anime_all))

print(f"Jumlah seluruh anime: {len(anime_all)}")

"""Kita memiliki 73515 data pengguna dari 12297 anime yang memiliki ratings.

## Menggabungkan Data dengan Fitur judul anime
"""

all_anime_rate= rating
all_anime_rate

all_anime_title=pd.merge(all_anime_rate,anime[['anime_id','name']],on="anime_id",how="left")
all_anime_title

"""## Menggabungkan Data dengan Fitur type anime"""

all_anime_type=pd.merge(all_anime_title,anime[['anime_id','type']],on="anime_id",how="left")
all_anime_type

"""## Menggabungkan Data dengan Fitur genre anime"""

all_anime=pd.merge(all_anime_type,anime[['anime_id','genre']],on="anime_id",how="left")
all_anime

"""# Data Preparation

## Mengatasi missing value
"""

all_anime.isnull().sum()

"""terdapat missing value pada judul anime, type anime dan genre anime. jumlah yang cukup banyak, tapi kalau menghapusnya kita masih punya banyak data yang tersisa."""

all_anime_clean = all_anime.dropna()

all_anime_clean.isnull().sum()

print(f"Jumlah data anime yang tersisa: {len(all_anime_clean['anime_id'].unique())}")
print(f'Jumlah data user yang tersisa: {len(all_anime_clean["user_id"].unique())}')

"""## Menghapus data duplikat"""

preparation = all_anime_clean.drop_duplicates('anime_id')

preparation.sort_values('anime_id')

"""## Konversi data series menjadi list"""

anime_id = preparation['anime_id'].tolist()
 
anime_title = preparation['name'].tolist()
 
# anime_type = preparation['type'].tolist()

anime_genre = preparation['genre'].tolist()
 
print(len(anime_id))
print(len(anime_title))
# print(len(anime_type))
print(len(anime_genre))

"""membuat dictionary untuk menentukan pasangan key-value pada data anime_id, anime_title, anime_type, anime_genre yang telah di siapkan sebelumnya."""

anime_new = pd.DataFrame({
    "anime_id":anime_id,
    "title":anime_title,
    # "type":anime_type,
    "genre":anime_genre
})

anime_new

"""# Modeling and Result

## Content based filtering
"""

data = anime_new
data.sample(5)

genre_anime = data["genre"].str.split(", | , | ,").astype(str)

"""### TF-IDF Vectorizer"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf_idf = TfidfVectorizer(token_pattern=r"\w{1,}")
tf_idf.fit(genre_anime)
tf_idf.get_feature_names()

tfidf_matrix = tf_idf.fit_transform(genre_anime)

tfidf_matrix.shape

tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf_idf.get_feature_names(),
    index=data.title
).sample(10, axis=1).sample(100, axis=0)

"""### Cosine Similarity"""

from sklearn.metrics.pairwise import cosine_similarity
 
cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim, index=data['title'], columns=data['title'])
print('Shape:', cosine_sim_df.shape)
 
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""### Mendapatkan Rekomendasi"""

def anime_recommendations(nama_anime, similarity_data=cosine_sim_df, items=data[['title', 'genre']], k=5):
    index = similarity_data.loc[:,nama_anime].to_numpy().argpartition(
        range(-1, -k, -1))
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(nama_anime, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

data[data.title.eq('One Punch Man')]

anime_recommendations('One Punch Man')

"""## Collaborative Filtering

### Data Understanding

import libary yang dibutuhkan untuk model collaborative filtering
"""

import pandas as pd
import numpy as np 
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""membaca dataset rating"""

df = rating
df

"""terdapat dataset dengan nilai -1"""

df.isnull().sum()

"""### Data Preparation

ubah nilai -1 menjadi 0.
"""

df[df['rating']==-1]

fix_df = df.replace(-1,0)
fix_df[fix_df['rating']==-1]

"""encode fitur user_id dan anime_id."""

user_ids = fix_df['user_id'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

anime_ids = fix_df['anime_id'].unique().tolist()

anime_id_to_anime_id_encoded = {x: i for i, x in enumerate(anime_ids)}

anime_id_encoded_to_anime_id = {i: x for i, x in enumerate(anime_ids)}

fix_df['user'] = fix_df['user_id'].map(user_to_user_encoded)
fix_df['anime'] = fix_df['anime_id'].map(anime_id_to_anime_id_encoded)

num_user = len(user_to_user_encoded)
print(num_user)

num_anime = len(anime_id_encoded_to_anime_id)
print(num_anime)

fix_df['rating'] = fix_df['rating'].values.astype(np.float32)

min_rating = min(fix_df['rating'])

max_rating = max(fix_df['rating'])

print(f'Number of User: {num_user}, Number of anime: {num_anime}, Min Rating: {min_rating}, Max Rating: {max_rating}')

"""### Membagi data train dan test"""

fix_df = fix_df.sample(frac=1,random_state=42)
fix_df

x = fix_df[['user','anime']].values

y = fix_df['rating'].apply(lambda x:(x-min_rating)/(max_rating-min_rating)).values

train_indices = int(0.8*df.shape[0])

x_train,x_val,y_train,y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x,y)

"""### Training"""

class RecommenderNet(tf.keras.Model):
  def __init__(self,num_user,num_anime,embedding_size,**kwargs):
    super(RecommenderNet,self).__init__(**kwargs)
    self.num_user = num_user
    self.num_anime = num_anime
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_user,
        embedding_size,
        embeddings_initializer= 'he_normal',
        embeddings_regularizer= keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_user,1)
    self.anime_embedding=layers.Embedding(
        num_anime,
        embedding_size,
        embeddings_initializer= 'he_normal',
        embeddings_regularizer= keras.regularizers.l2(1e-6)
    )
    self.anime_bias = layers.Embedding(num_anime,1)

  def call(self,inputs):
    user_vector = self.user_embedding(inputs[:,0]) #Embedding layer 1
    user_bias = self.user_bias(inputs[:,0])#Layer 2
    anime_vector = self.anime_embedding(inputs[:,1]) #Layer 3
    anime_bias = self.anime_bias(inputs[:,1]) # Layer 4

    dot_user_anime = tf.tensordot(user_vector,anime_vector,2)

    x = dot_user_anime+user_bias+anime_bias
    return tf.nn.sigmoid(x) # Aktivasi sigmoid

model = RecommenderNet(num_user,num_anime,50) #inisialisasi model

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = tf.optimizers.SGD(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=128,
    epochs=5,
    validation_data=(x_val,y_val)
)

"""### Visualisasi Metrik"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### Mendapatkan Rekomendasi Resto"""

anime_df = anime_new
df = pd.read_csv('sample_data/rating.csv')
 
user_id = df.user_id.sample(1).iloc[0]
anime_watched_by_user = df[df.user_id == user_id]
 
anime_not_watched = anime_df[~anime_df['anime_id'].isin(anime_watched_by_user.anime_id.values)]['anime_id'] 
anime_not_watched = list(
    set(anime_not_watched)
    .intersection(set(anime_id_encoded_to_anime_id.keys()))
)
 
anime_not_watched = [[anime_id_encoded_to_anime_id.get(x)] for x in anime_not_watched]
user_encoder = user_to_user_encoded.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_watched), anime_not_watched)
)

ratings = model.predict(user_anime_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime_ids = [
    anime_id_encoded_to_anime_id.get(anime_not_watched[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('anime with high ratings from user')
print('----' * 8)
 
top_anime_user = (
    anime_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .anime_id.values
)
 
anime_df_rows = anime_df[anime_df['anime_id'].isin(top_anime_user)]
for row in anime_df_rows.itertuples():
    print(row.title, ':', row.genre)
 
print('----' * 8)
print('Top 10 anime recommendation')
print('----' * 8)
 
recommended_anime = anime_df[anime_df['anime_id'].isin(recommended_anime_ids)]
for row in recommended_anime.itertuples():
    print(row.title, ':', row.genre)

"""### Evaluasi"""

from sklearn.metrics import mean_squared_error
from math import sqrt

data = {
    "train":sqrt(mean_squared_error(y_train, model.predict(x_train).flatten())),
    "test":sqrt(mean_squared_error(y_val, model.predict(x_val).flatten()))
}

rmse = pd.DataFrame(data=data, index=['Collaborative filtering'])
rmse

fig, ax = plt.subplots()
rmse.sort_values(by='test', ascending=False).plot(kind='bar', ax=ax, zorder=3)
ax.grid(zorder=0)